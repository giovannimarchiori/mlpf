{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from math import sqrt\n",
    "sys.path.append(\"/afs/cern.ch/work/m/mgarciam/private/mlpf/\")\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, num_pts=-1, shuffle=True):\n",
    "        self.data = data\n",
    "\n",
    "        if num_pts > len(data[\"Nobj\"]):\n",
    "            raise ValueError(\n",
    "                \"Desired number of points ({}) is greater than the number of data points ({}) available in the dataset!\".format(\n",
    "                    num_pts, len(data[\"Nobj\"])\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if num_pts < 0:\n",
    "            self.num_pts = len(data[\"Nobj\"])\n",
    "        else:\n",
    "            self.num_pts = num_pts\n",
    "\n",
    "        if shuffle:\n",
    "            g = torch.Generator().manual_seed(42)\n",
    "            self.perm = torch.randperm(len(data[\"Nobj\"]), generator=g)[: self.num_pts]\n",
    "        else:\n",
    "            self.perm = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.perm is not None:\n",
    "            idx = self.perm[idx]\n",
    "        return {key: val[idx] for key, val in self.data.items()}\n",
    "\n",
    "\n",
    "def initialize_datasets(datadir=\"./data\", num_pts=None):\n",
    "    \"\"\"\n",
    "    Initialize datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    ### ------ 1: Get the file names ------ ###\n",
    "    # datadir should be the directory in which the HDF5 files (e.g. out_test.h5, out_train.h5, out_valid.h5) reside.\n",
    "    # There may be many data files, in some cases the test/train/validate sets may themselves be split across files.\n",
    "    # We will look for the keywords defined in splits to be be in the filenames, and will thus determine what\n",
    "    # set each file belongs to.\n",
    "    splits = [\n",
    "        \"train\",\n",
    "        \"test\",\n",
    "        \"valid\",\n",
    "    ]  # Our data categories -- training set, testing set and validation set\n",
    "    patterns = {\n",
    "        \"train\": \"train\",\n",
    "        \"test\": \"test\",\n",
    "        \"valid\": \"val\",\n",
    "    }  # Patterns to look for in data files, to identify which data category each belongs in\n",
    "\n",
    "    files = glob.glob(datadir + \"/*.h5\")\n",
    "    assert len(files) > 0, f\"Could not find any HDF5 files in the directory {datadir}!\"\n",
    "    datafiles = {split: [] for split in splits}\n",
    "    for file in files:\n",
    "        for split, pattern in patterns.items():\n",
    "            if pattern in file:\n",
    "                datafiles[split].append(file)\n",
    "    nfiles = {split: len(datafiles[split]) for split in splits}\n",
    "    print(nfiles)\n",
    "    ### ------ 2: Set the number of data points ------ ###\n",
    "    # There will be a JetDataset for each file, so we divide number of data points by number of files,\n",
    "    # to get data points per file. (Integer division -> must be careful!)\n",
    "    # TODO: nfiles > npoints might cause issues down the line, but it's an absurd use case\n",
    "\n",
    "    if num_pts is None:\n",
    "        num_pts = {\"train\": -1, \"test\": -1, \"valid\": -1}\n",
    "    num_pts = {\"train\": -1, \"test\": -1, \"valid\": 1}\n",
    "    num_pts_per_file = {}\n",
    "    for split in splits:\n",
    "        num_pts_per_file[split] = []\n",
    "\n",
    "        if num_pts[split] == -1:\n",
    "            for n in range(nfiles[split]):\n",
    "                num_pts_per_file[split].append(-1)\n",
    "        else:\n",
    "            for n in range(nfiles[split]):\n",
    "                print(\"split\", split)\n",
    "                num_pts_per_file[split].append(\n",
    "                    int(np.ceil(num_pts[split] / nfiles[split]))\n",
    "                )\n",
    "            num_pts_per_file[split][-1] = int(\n",
    "                np.maximum(\n",
    "                    num_pts[split] - np.sum(np.array(num_pts_per_file[split])[0:-1]), 0\n",
    "                )\n",
    "            )\n",
    "\n",
    "    ### ------ 3: Load the data ------ ###\n",
    "    datasets = {}\n",
    "    for split in splits:\n",
    "        print(f\"Loading {split} data...\")\n",
    "        datasets[split] = []\n",
    "        for file in datafiles[split]:\n",
    "            print(file)\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                datasets[split].append(\n",
    "                    {\n",
    "                        key: torch.from_numpy(val)\n",
    "                        for key, val in f.items()\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "\n",
    "    ### ------ 4: Error checking ------ ###\n",
    "    # Basic error checking: Check the training/test/validation splits have the same set of keys.\n",
    "    # keys = []\n",
    "    # for split in splits:\n",
    "    #     for dataset in datasets[split]:\n",
    "    #         keys.append(dataset.keys())\n",
    "    # assert all([key == keys[0] for key in keys]), \"Datasets must have same set of keys!\"\n",
    "\n",
    "    ### ------ 5: Initialize datasets ------ ###\n",
    "    # Now initialize datasets based upon loaded data\n",
    "    torch_datasets = {\n",
    "        split: ConcatDataset(\n",
    "            [\n",
    "                JetDataset(data, num_pts=num_pts_per_file[split][idx])\n",
    "                for idx, data in enumerate(datasets[split])\n",
    "            ]\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    "\n",
    "    return torch_datasets\n",
    "\n",
    "\n",
    "def batch_stack_general(props):\n",
    "    \"\"\"\n",
    "    Stack a list of torch.tensors so they are padded to the size of the\n",
    "    largest tensor along each axis.\n",
    "\n",
    "    Unlike :batch_stack:, this will automatically stack scalars, vectors,\n",
    "    and matrices. It will also automatically convert Numpy Arrays to\n",
    "    Torch Tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    props : list or tuple of Pytorch Tensors, Numpy ndarrays, ints or floats.\n",
    "        Pytorch tensors to stack\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    props : Pytorch tensor\n",
    "        Stacked pytorch tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    TODO : Review whether the behavior when elements are not tensors is safe.\n",
    "    \"\"\"\n",
    "    if type(props[0]) in [int, float]:\n",
    "        # If batch is list of floats or ints, just create a new Torch Tensor.\n",
    "        return torch.tensor(props)\n",
    "\n",
    "    if type(props[0]) is np.ndarray:\n",
    "        # Convert numpy arrays to tensors\n",
    "        props = [torch.from_numpy(prop) for prop in props]\n",
    "\n",
    "    shapes = [prop.shape for prop in props]\n",
    "\n",
    "    if all(shapes[0] == shape for shape in shapes):\n",
    "        # If all shapes are the same, stack along dim=0\n",
    "        return torch.stack(props)\n",
    "\n",
    "    elif all(shapes[0][1:] == shape[1:] for shape in shapes):\n",
    "        # If shapes differ only along first axis, use the RNN pad_sequence to pad/stack.\n",
    "        return torch.nn.utils.rnn.pad_sequence(props, batch_first=True, padding_value=0)\n",
    "\n",
    "    elif all((shapes[0][2:] == shape[2:]) for shape in shapes):\n",
    "        # If shapes differ along the first two axes, (shuch as a matrix),\n",
    "        # pad/stack first two axes\n",
    "\n",
    "        # Ensure that input features are matrices\n",
    "        assert all(\n",
    "            (shape[0] == shape[1]) for shape in shapes\n",
    "        ), \"For batch stacking matrices, first two indices must match for every data point\"\n",
    "\n",
    "        max_atoms = max([len(p) for p in props])\n",
    "        max_shape = (len(props), max_atoms, max_atoms) + props[0].shape[2:]\n",
    "        padded_tensor = torch.zeros(\n",
    "            max_shape, dtype=props[0].dtype, device=props[0].device\n",
    "        )\n",
    "\n",
    "        for idx, prop in enumerate(props):\n",
    "            this_atoms = len(prop)\n",
    "            padded_tensor[idx, :this_atoms, :this_atoms] = prop\n",
    "\n",
    "        return padded_tensor\n",
    "    else:\n",
    "        ValueError(\n",
    "            \"Input tensors must have the same shape on all but at most the first two axes!\"\n",
    "        )\n",
    "\n",
    "\n",
    "def batch_stack(props, edge_mat=False, nobj=None):\n",
    "    \"\"\"\n",
    "    Stack a list of torch.tensors so they are padded to the size of the\n",
    "    largest tensor along each axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    props : list of Pytorch Tensors\n",
    "        Pytorch tensors to stack\n",
    "    edge_mat : bool\n",
    "        The included tensor refers to edge properties, and therefore needs\n",
    "        to be stacked/padded along two axes instead of just one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    props : Pytorch tensor\n",
    "        Stacked pytorch tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    TODO : Review whether the behavior when elements are not tensors is safe.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(props[0]):\n",
    "        return torch.tensor(props)\n",
    "    elif props[0].dim() == 0:\n",
    "        return torch.stack(props)\n",
    "    elif not edge_mat:\n",
    "        props = [p[:nobj, ...] for p in props]\n",
    "        return torch.nn.utils.rnn.pad_sequence(props, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        max_atoms = max([len(p) for p in props])\n",
    "        max_shape = (len(props), max_atoms, max_atoms) + props[0].shape[2:]\n",
    "        padded_tensor = torch.zeros(\n",
    "            max_shape, dtype=props[0].dtype, device=props[0].device\n",
    "        )\n",
    "\n",
    "        for idx, prop in enumerate(props):\n",
    "            this_atoms = len(prop)\n",
    "            padded_tensor[idx, :this_atoms, :this_atoms] = prop\n",
    "\n",
    "        return padded_tensor\n",
    "\n",
    "\n",
    "def drop_zeros(props, to_keep):\n",
    "    \"\"\"\n",
    "    Function to drop zeros from batches when the entire dataset is padded to the largest molecule size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    props : Pytorch tensor\n",
    "        Full Dataset\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    props : Pytorch tensor\n",
    "        The dataset with  only the retained information.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    TODO : Review whether the behavior when elements are not tensors is safe.\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(props[0]):\n",
    "        return props\n",
    "    elif props[0].dim() == 0 or props[0].shape[0] != to_keep.shape[0]:\n",
    "        return props\n",
    "    else:\n",
    "        return props[:, to_keep, ...]\n",
    "\n",
    "\n",
    "def normsq4(p):\n",
    "    # Quick hack to calculate the norms of the four-vectors\n",
    "    # The last dimension of the input gets eaten up\n",
    "    psq = torch.pow(p, 2)\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "\n",
    "enc = OneHotEncoder().fit([[-1], [1]])\n",
    "\n",
    "\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx * n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    data, scale=1.0, nobj=None, edge_features=[], add_beams=False, beam_mass=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Collation function that collates datapoints into the batch format for lgn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of datapoints\n",
    "        The data to be collated.\n",
    "    edge_features : list of strings\n",
    "        Keys of properties that correspond to edge features, and therefore are\n",
    "        matrices of shapes (num_atoms, num_atoms), which when forming a batch\n",
    "        need to be padded along the first two axes instead of just the first one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch : dict of Pytorch tensors\n",
    "        The collated data.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        prop: batch_stack([mol[prop] for mol in data], nobj=nobj)\n",
    "        for prop in data[0].keys()\n",
    "    }\n",
    "    data[\"label\"] = data[\"label\"].to(torch.bool)\n",
    "\n",
    "    # to_keep = batch['Nobj'].to(torch.uint8)\n",
    "    to_keep = torch.any(data[\"label\"], dim=0)\n",
    "    data = {key: drop_zeros(prop, to_keep) for key, prop in data.items()}\n",
    "\n",
    "    if add_beams:\n",
    "        beams = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [sqrt(1 + beam_mass**2), 0, 0, 1],\n",
    "                    [sqrt(1 + beam_mass**2), 0, 0, -1],\n",
    "                ]\n",
    "            ],\n",
    "            dtype=data[\"Pmu\"].dtype,\n",
    "        ).expand(data[\"Pmu\"].shape[0], 2, 4)\n",
    "        s = data[\"Pmu\"].shape\n",
    "        data[\"Pmu\"] = torch.cat([beams * scale, data[\"Pmu\"] * scale], dim=1)\n",
    "        labels = torch.cat((torch.ones(s[0], 2), -torch.ones(s[0], s[1])), dim=1)\n",
    "        if \"scalars\" not in data.keys():\n",
    "            data[\"scalars\"] = labels.to(dtype=data[\"Pmu\"].dtype).unsqueeze(-1)\n",
    "        else:\n",
    "            data[\"scalars\"] = torch.stack(\n",
    "                (data[\"scalars\"], labels.to(dtype=data[\"Pmu\"].dtype))\n",
    "            )\n",
    "    else:\n",
    "        data[\"Pmu\"] = data[\"Pmu\"] * scale\n",
    "\n",
    "    # batch = {key: drop_zeros(prop, to_keep) for key, prop in batch.items()}\n",
    "    atom_mask = data[\"Pmu\"][..., 0] != 0.0\n",
    "    # Obtain edges\n",
    "    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "\n",
    "    # mask diagonal\n",
    "    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    edge_mask *= diag_mask\n",
    "\n",
    "    data[\"atom_mask\"] = atom_mask.to(torch.bool)\n",
    "    data[\"edge_mask\"] = edge_mask.to(torch.bool)\n",
    "\n",
    "    batch_size, n_nodes, _ = data[\"Pmu\"].size()\n",
    "\n",
    "    # Centralize Data\n",
    "    # data['Pmu'] = data['Pmu'] - data['Pmu'].sum(dim=1, keepdim=True) / data['Nobj'][:,None,None]\n",
    "\n",
    "    if add_beams:\n",
    "        beamlabel = data[\"scalars\"]\n",
    "        one_hot = (\n",
    "            enc.transform(beamlabel.reshape(-1, 1))\n",
    "            .toarray()\n",
    "            .reshape(batch_size, n_nodes, -1)\n",
    "        )\n",
    "        one_hot = torch.tensor(one_hot)\n",
    "\n",
    "        mass = normsq4(data[\"Pmu\"]).abs().sqrt().unsqueeze(-1)  # [B,N,1]\n",
    "        mass_tensor = mass.view(mass.shape + (1,))\n",
    "        nodes = (one_hot.unsqueeze(-1) * mass_tensor).view(\n",
    "            mass.shape[:2] + (-1,)\n",
    "        )  # [B,N,2]\n",
    "    else:\n",
    "        mass = normsq4(data[\"Pmu\"]).unsqueeze(-1)\n",
    "        nodes = mass\n",
    "\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, data[\"edge_mask\"])\n",
    "    data[\"nodes\"] = nodes\n",
    "    data[\"edges\"] = edges\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def retrieve_dataloaders(batch_size, num_workers=4, num_train=-1, datadir=\"./data\"):\n",
    "    # Initialize dataloader\n",
    "    datadir = \"/home/druhe/github/LorentzNet-release/data/top/\"\n",
    "    datasets = initialize_datasets(\n",
    "        datadir, num_pts={\"train\": num_train, \"test\": -1, \"valid\": -1}\n",
    "    )\n",
    "    # distributed training\n",
    "    # train_sampler = DistributedSampler(datasets['train'])\n",
    "    # Construct PyTorch dataloaders from datasets\n",
    "    collate = lambda data: collate_fn(data, scale=1, add_beams=True, beam_mass=1)\n",
    "    dataloaders = {\n",
    "        split: DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size\n",
    "            if (split == \"train\")\n",
    "            else batch_size,  # prevent CUDA memory exceeded\n",
    "            # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            drop_last=True if (split == \"train\") else False,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    }\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "class TopTaggingDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=32,\n",
    "        num_train=1024,\n",
    "        num_val=1024,\n",
    "        num_test=1024,\n",
    "        # num_workers=4,\n",
    "    ) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        # self.num_workers = num_workers\n",
    "        datadir = os.path.join(os.environ[\"DATAROOT\"], \"top_tagging\")\n",
    "        self.datasets = initialize_datasets(\n",
    "            datadir, num_pts={\"train\": num_train, \"valid\": num_val, \"test\": num_test}\n",
    "        )\n",
    "        self.collate = lambda data: collate_fn(\n",
    "            data, scale=1, add_beams=True, beam_mass=1\n",
    "        )\n",
    "\n",
    "    def train_loader(self):\n",
    "        distributed = torch.distributed.is_initialized()\n",
    "        sampler = DistributedSampler(self.datasets[\"train\"]) if distributed else None\n",
    "        shuffle = False if distributed else True\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            # pin_memory=True,\n",
    "            # persistent_workers=True,\n",
    "            drop_last=True,\n",
    "            # num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "        )\n",
    "\n",
    "    def val_loader(self):\n",
    "        distributed = torch.distributed.is_initialized()\n",
    "        sampler = DistributedSampler(self.datasets[\"valid\"]) if distributed else None\n",
    "        return DataLoader(\n",
    "            self.datasets[\"valid\"],\n",
    "            batch_size=self.batch_size,\n",
    "            # pin_memory=True,\n",
    "            # persistent_workers=True,\n",
    "            drop_last=True,\n",
    "            # num_workers=self.num_workers,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "    def test_loader(self):\n",
    "        distributed = torch.distributed.is_initialized()\n",
    "        sampler = DistributedSampler(self.datasets[\"test\"]) if distributed else None\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            # pin_memory=True,\n",
    "            # persistent_workers=True,\n",
    "            drop_last=True,\n",
    "            # num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "            sampler=sampler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0, 'test': 0, 'valid': 1}\n",
      "split valid\n",
      "Loading train data...\n",
      "Loading test data...\n",
      "Loading valid data...\n",
      "/afs/cern.ch/work/m/mgarciam/private/mlpf/notebooks/val.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Group)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# self.num_workers = num_workers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m datadir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/afs/cern.ch/work/m/mgarciam/private/mlpf/notebooks/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m datasets \u001b[39m=\u001b[39m initialize_datasets(datadir, num_pts\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mNone\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m: num_val, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mNone\u001b[39;49;00m})\n\u001b[1;32m      9\u001b[0m collate \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m data: collate_fn(\n\u001b[1;32m     10\u001b[0m             data, scale\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, add_beams\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, beam_mass\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m distributed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 115\u001b[0m, in \u001b[0;36minitialize_datasets\u001b[0;34m(datadir, num_pts)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mprint\u001b[39m(file)\n\u001b[1;32m    113\u001b[0m         \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    114\u001b[0m             datasets[split]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 115\u001b[0m                 {\n\u001b[1;32m    116\u001b[0m                     key: torch\u001b[39m.\u001b[39mfrom_numpy(val)\n\u001b[1;32m    117\u001b[0m                     \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    118\u001b[0m                 }\n\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    122\u001b[0m \u001b[39m### ------ 4: Error checking ------ ###\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# Basic error checking: Check the training/test/validation splits have the same set of keys.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# keys = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m### ------ 5: Initialize datasets ------ ###\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# Now initialize datasets based upon loaded data\u001b[39;00m\n\u001b[1;32m    132\u001b[0m torch_datasets \u001b[39m=\u001b[39m {\n\u001b[1;32m    133\u001b[0m     split: ConcatDataset(\n\u001b[1;32m    134\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits\n\u001b[1;32m    140\u001b[0m }\n",
      "Cell \u001b[0;32mIn[30], line 116\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mprint\u001b[39m(file)\n\u001b[1;32m    113\u001b[0m         \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    114\u001b[0m             datasets[split]\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    115\u001b[0m                 {\n\u001b[0;32m--> 116\u001b[0m                     key: torch\u001b[39m.\u001b[39;49mfrom_numpy(val)\n\u001b[1;32m    117\u001b[0m                     \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    118\u001b[0m                 }\n\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    122\u001b[0m \u001b[39m### ------ 4: Error checking ------ ###\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# Basic error checking: Check the training/test/validation splits have the same set of keys.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# keys = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m### ------ 5: Initialize datasets ------ ###\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# Now initialize datasets based upon loaded data\u001b[39;00m\n\u001b[1;32m    132\u001b[0m torch_datasets \u001b[39m=\u001b[39m {\n\u001b[1;32m    133\u001b[0m     split: ConcatDataset(\n\u001b[1;32m    134\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits\n\u001b[1;32m    140\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Group)"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=32,\n",
    "num_train=1024,\n",
    "num_val=1024,\n",
    "num_test=1024,\n",
    "        \n",
    "# self.num_workers = num_workers\n",
    "datadir = \"/afs/cern.ch/work/m/mgarciam/private/mlpf/notebooks/\"\n",
    "datasets = initialize_datasets(datadir, num_pts={\"train\": None, \"valid\": num_val, \"test\": None})\n",
    "collate = lambda data: collate_fn(\n",
    "            data, scale=1, add_beams=True, beam_mass=1\n",
    "        )\n",
    "distributed = False\n",
    "sampler = DistributedSampler(datasets[\"valid\"]) if distributed else None\n",
    "dataloader_val = DataLoader(\n",
    "datasets[\"valid\"],\n",
    "batch_size=batch_size,\n",
    "# pin_memory=True,\n",
    "# persistent_workers=True,\n",
    "drop_last=True,\n",
    "# num_workers=self.num_workers,\n",
    "sampler=sampler,\n",
    "collate_fn=collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table\n",
      "<HDF5 group \"/table\" (2 members)>\n"
     ]
    }
   ],
   "source": [
    "file = \"/afs/cern.ch/work/m/mgarciam/private/mlpf/notebooks/val.h5\"\n",
    "with h5py.File(file, \"r\") as f:\n",
    "    for key, val in f.items():\n",
    "        print(key)\n",
    "        print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid group (or file) id (invalid group (or file) ID)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    709\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/h5py/_hl/base.py:388\u001b[0m, in \u001b[0;36mKeysViewHDF5.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m<KeysViewHDF5 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m))\n",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/_collections_abc.py:866\u001b[0m, in \u001b[0;36mMappingView.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 866\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/h5py/_hl/group.py:494\u001b[0m, in \u001b[0;36mGroup.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m@with_phil\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Number of members attached to this group \"\"\"\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid\u001b[39m.\u001b[39;49mget_num_objs()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5g.pyx:336\u001b[0m, in \u001b[0;36mh5py.h5g.GroupID.get_num_objs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid group (or file) id (invalid group (or file) ID)"
     ]
    }
   ],
   "source": [
    "val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('graphgps': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1db199df8f75d900d458855decbcf5956490222a21736f6416c2999d256400d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
